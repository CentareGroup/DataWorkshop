{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization Developer Workshop\n",
    "\n",
    "In this data visualization workshop, we will be analyzing a real-world time series dataset to predict the future. This notebook demonstrates how to download and prepare a live dataset, create data visualizations, and train and deploy a DeepAR model on the AWS SageMaker platform.\n",
    "\n",
    "### Traffic Accident Prediction\n",
    "In our hypothetical scenario, we are trying to predict the risk of traffic accidents in the City of Milwaukee. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Setup\n",
    "\n",
    "Import the modules we will be using in our notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i helper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Download and Preparation\n",
    "\n",
    "We will first download our [datasource](https://data.milwaukee.gov/dataset/trafficaccident) from the [City of Milwaukee Data Portal](https://data.milwaukee.gov/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://data.milwaukee.gov/dataset/5fafe01d-dc55-4a41-8760-8ae52f7855f1/resource/8fffaa3a-b500-4561-8898-78a424bdacee/download/trafficaccident.csv\"\n",
    "filename = \"trafficaccident.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(filename):\n",
    "    print(\"downloading dataset, can take a few minutes depending on your connection\")\n",
    "    urlretrieve(url, filename, reporthook=progress_report_hook)\n",
    "else:\n",
    "    print(\"File found skipping download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we load and parse the dataset and convert it to a collection of Pandas time series, which makes common time series operations such as indexing by time periods or resampling much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df = pd.read_csv(filename, sep=\",\", parse_dates=True, error_bad_lines=False)\n",
    "#csv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_series = pd.to_datetime(csv_df['CASEDATE'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "ts_series = ts_series.sort_values()\n",
    "ts_series = ts_series.dropna()\n",
    "ts_series = ts_series.rename(\"timestamp\")\n",
    "#ts_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since accidents are recorded individually in the original dataset, we need to aggregate the count of accidents into 1 hour bins. Also, for this particular dataset, data prior to 2008 is incomplete. We will only keeps records starting at 1/1/2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(index=ts_series.rename(\"timestamp\"))\n",
    "df['count'] = 1\n",
    "df = df.resample('1H').sum()\n",
    "df = df['2008':]\n",
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following visualizations, we want with full years, 2008 - 2018 in our case. We will create a dataframe for our target year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2008_2018 = df['???':'???']\n",
    "#df_2008_2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Accidents Over Time\n",
    "\n",
    "We will start with two simple line graphs. First graph will display the number of accidents over the entire set of data, allowing us to see trends spanning multiple years. Next the graph will overlay each year on one another, showing how years compare to one another as well as showing seasonality trends.\n",
    "\n",
    "#### Steps\n",
    "\n",
    "\n",
    "1. Resampling the dataframe will be necessary to show meaningful results. Experiment with no resampling and by the hour, week, month, etc. \n",
    "\n",
    "1. Next create a pivot table dataframe, with the month as the index and the years the columns. Aggregate using the sum function. \n",
    "\n",
    "1. Plot the resampled dataframe.\n",
    "\n",
    "1. Plot the pivot dataframe.\n",
    "\n",
    "1. Use the plt.show() to output graph\n",
    "\n",
    "#### Notes\n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html  \n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.resample.html  \n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html\n",
    "\n",
    "Frequencies used in reample function:\n",
    "\n",
    "    Alias   Description\n",
    "    D       calendar day frequency\n",
    "    W       weekly frequency\n",
    "    M       month end frequency\n",
    "    MS      month start frequency\n",
    "    Q       quarter end frequency\n",
    "    QS      quarter start frequency\n",
    "    A       year end frequency\n",
    "    AS      year start frequency\n",
    "    H       hourly frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1\n",
    "df_resample = df_2008_2018.resample('??').sum()\n",
    "\n",
    "#Step 2\n",
    "df_pivot = pd.pivot_table(\n",
    "    df_2008_2018,\n",
    "    index = df_2008_2018.index.month,\n",
    "    columns = df_2008_2018.index.year,\n",
    "    aggfunc = 'sum'\n",
    ")\n",
    "\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Step 3\n",
    "# Plot df_resample using the plot() function \n",
    "\n",
    "#Step 4\n",
    "# TODO: plot df_pivot using the plot() function\n",
    "\n",
    "\n",
    "#Moving the legend to the upper right\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "\n",
    "#Step 5\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms \n",
    "A histogram groups numbers into ranges, generally represented as a bar chart. A histogram is a plot that lets you discover, and show, the underlying frequency distribution (shape) of a set of continuous data.  \n",
    "\n",
    "\n",
    "#### Steps\n",
    "1. First create dataframes of your sample data grouped by hour or day, day of week, and month of year\n",
    "2. Plot the grouped by dataframe, with a kind of 'bar'\n",
    "\n",
    "#### Notes\n",
    "Monday = 0  \n",
    "Midnight - 1:00AM = 0  \n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html  \n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/series.html#datetime-properties  \n",
    "https://matplotlib.org/api/_as_gen/matplotlib.pyplot.imshow.html  \n",
    "https://matplotlib.org/tutorials/colors/colormaps.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1\n",
    "df_groupedby_hour = df_2008_2018.groupby(df_2008_2018.index.hour).sum();\n",
    "df_groupedby_weekday = # TODO: try the same, but with weekday\n",
    "df_groupedby_month = # TODO: try the same, but with month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Step 2\n",
    "df_groupedby_hour.plot(kind='bar')\n",
    "# TODO: plot the weekday\n",
    "# TODO: plot the month\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add some labels to make the graphs easier to understand. There are a couple of different way to accomplish this. For our example, we will rename index and columns of our grouped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groupedby_hour.rename(columns={'count':'Accidents'}, inplace=True)\n",
    "df_groupedby_hour.index.name = #TODO: Choose an x-axis title\n",
    "\n",
    "df_groupedby_weekday.rename(columns={'count':'Accidents'}, inplace=True)\n",
    "df_groupedby_weekday.index.name = #TODO: Choose an x-axis title\n",
    "\n",
    "df_groupedby_month.rename(columns={'count':'Accidents'}, inplace=True)                          \n",
    "df_groupedby_month.index.name = #TODO: Choose an x-axis title\n",
    "\n",
    "#TODO: plot the graphs again (hint: you can copy and past from last code block)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets try to make the charts more compact. We will do this by creating subplots. Subplot are created on a grid system. We will use the plt.subplot() which creates a single subplot within a grid. this command takes three integer arguments: the number of rows, the number of columns, and the index of the plot to be created in this scheme, which runs from the upper left to the bottom right.\n",
    "\n",
    "Our subplot grid will have a height of 1 and width of 3.\n",
    "\n",
    "    add_subplot(1,3 x)\n",
    "   \n",
    "    +---+---+---+\n",
    "    | 1 | 2 | 3 | <== x\n",
    "    +---+---+---+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(1, figsize=(15, 4))\n",
    "\n",
    "# Divide the figure into a 1x3 grid, and give me the first section\n",
    "ax1 = fig.add_subplot(1,3,1)\n",
    "\n",
    "# Divide the figure into a 1x3 grid, and give me the second section\n",
    "ax2 = fig.add_subplot(1,3,2)\n",
    "\n",
    "# Divide the figure into a 1x3 grid, and give me the third section\n",
    "ax3 = fig.add_subplot(1,3,3)\n",
    "\n",
    "df_groupedby_hour.plot(kind='bar', ax=ax3)\n",
    "df_groupedby_weekday.plot(kind='bar',ax=ax2)\n",
    "df_groupedby_month.plot(kind='bar',ax=ax1)\n",
    "\n",
    "#TODO: Try to rearange the order of th graphs\n",
    "\n",
    "plt.suptitle('2008-2018 Accidents')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heat Map Graph\n",
    "Let's create a heat map, comparing the accidents that occur on each day of the week by hour. A heat map displays the values of matrix in a two dimensions.\n",
    "\n",
    "#### Steps\n",
    "1. First create a dataframe of your sample data.\n",
    "2. Next create a pivot table dataframe, with the day of the week as the index (y axis) and hour of the day as the column (x axis). Aggregate using the mean function. \n",
    "3. Plot the pivot dataframe as a heat map using the `imshow()` function. \n",
    "\n",
    "#### Notes\n",
    "Monday = 0  \n",
    "Midnight - 1:00AM = 0  \n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html  \n",
    "https://matplotlib.org/api/_as_gen/matplotlib.pyplot.imshow.html  \n",
    "https://matplotlib.org/tutorials/colors/colormaps.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Step 1\n",
    "df_heatmap = #TODO: use the df[ ?? : ?? ] syntax to select just 2018\n",
    "\n",
    "#Step 2\n",
    "pv_heatmap = pd.pivot_table(\n",
    "    df_heatmap, \n",
    "    index = df_heatmap.index.weekday,\n",
    "    columns = df_heatmap.index.hour,\n",
    "    aggfunc='mean')\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "#Step 3\n",
    "im = plt.imshow(pv_heatmap,cmap='jet')\n",
    "\n",
    "#TODO: Try to change the color to 'plasma' or another color from this link: https://matplotlib.org/tutorials/colors/colormaps.html\n",
    "#TODO: adjust the size of the heat map by change the parameters of the plt.figure(...) function\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, we will configure the values to configure the S3 buckets.\n",
    "\n",
    "The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting. \n",
    "\n",
    "The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = sagemaker.Session().default_bucket()  # replace with an existing bucket if needed\n",
    "s3_prefix = 'deepar-traffic-accident-notebook'    # prefix used for all data stored within the bucket\n",
    "s3_uid = #TODO: Set a unique id                   # unique ID used for S3 data storage location\n",
    "\n",
    "role = sagemaker.get_execution_role()             # IAM role to use by SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "s3_data_path = \"s3://{}/uid_{}/{}/data\".format(s3_bucket, s3_uid, s3_prefix)\n",
    "s3_output_path = \"s3://{}/uid_{}/{}/output\".format(s3_bucket, s3_uid, s3_prefix)\n",
    "# print(s3_data_path)\n",
    "# print(s3_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Test Data\n",
    "\n",
    "To configure DeepAR, we need to split the data into training and testing data. For the training data, we will use data from 2008 through 2017, split by year. The training data will be all of 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_training = []\n",
    "time_series_training.append(df[\"2008\":\"2008\"]['count'])\n",
    "time_series_training.append(df[\"2009\":\"2009\"]['count'])\n",
    "time_series_training.append(df[\"2010\":\"2010\"]['count'])\n",
    "time_series_training.append(df[\"2011\":\"2011\"]['count'])\n",
    "time_series_training.append(df[\"2012\":\"2012\"]['count'])\n",
    "time_series_training.append(df[\"2013\":\"2013\"]['count'])\n",
    "time_series_training.append(df[\"2014\":\"2014\"]['count'])\n",
    "time_series_training.append(df[\"2015\":\"2015\"]['count'])\n",
    "time_series_training.append(df[\"2016\":\"2016\"]['count'])\n",
    "time_series_training.append(df[\"2017\":\"2017\"]['count'])\n",
    "\n",
    "time_series_test = []\n",
    "time_series_test.append(df['2018':'2018']['count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write training and test data to S3\n",
    "\n",
    "Now that we have the data arrays prepared, let us copy them to S3 where DeepAR can access them. This may take a couple of minutes, depending on your connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = \"utf-8\"\n",
    "s3filesystem = s3fs.S3FileSystem()\n",
    "\n",
    "with s3filesystem.open(s3_data_path + \"/train/train.json\", 'wb') as fp:\n",
    "    for ts in time_series_training:\n",
    "        fp.write(series_to_jsonline(ts).encode(encoding))\n",
    "        fp.write('\\n'.encode(encoding))\n",
    "\n",
    "with s3filesystem.open(s3_data_path + \"/test/test.json\", 'wb') as fp:\n",
    "    for ts in time_series_test:\n",
    "        fp.write(series_to_jsonline(ts).encode(encoding))\n",
    "        fp.write('\\n'.encode(encoding))\n",
    "        \n",
    "#TODO - Navigate to S3 using the following link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-963348920600/?region=us-east-1&tab=overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure DeepAR estimator training job\n",
    "\n",
    "We can now define the estimator that will launch the training job and configure the container image to be used for the region that we are running in.\n",
    "#### Notes\n",
    "https://sagemaker.readthedocs.io/en/stable/estimators.html#sagemaker.estimator.Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "image_name = get_image_uri(boto3.Session().region_name, 'forecasting-deepar')\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.xlarge',\n",
    "    base_job_name='DEMO-deepar',\n",
    "    output_path=s3_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Hyperparameters\n",
    "algorithm\n",
    "Hyperparameters are parameters used for to configure machine learning algorithm, set are outside prior to the training process. Changing these parameters can have a dramatic impact on the accuracy and performance of the model. That said, tuning most of these hyperparameters is outside the scope of this workshop. The relevant hyperparameters are as follows:\n",
    "\n",
    " * **time_freq** - The target frequency interval of the data. Hourly in this example.\n",
    " * **context_length** - How far in the past DeepAR's network can see. Higher values may result in more accuracy at the expense of training time and expense, and computing resources to execute model.\n",
    " * **prediction_length** - How far in the future DeepAR's network can predict. Like **context_length**, higher values are more taxing during model training and execution. \n",
    "\n",
    "#### Notes\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_freq = 'H'\n",
    "prediction_length = 336 # 2 weeks (24 * 14)\n",
    "context_length = 336"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": time_freq,\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "    \"num_cells\": \"55\",\n",
    "    \"num_layers\": \"2\",\n",
    "    \"likelihood\": \"gaussian\",\n",
    "    \"epochs\": \"40\",\n",
    "    \"mini_batch_size\": \"33\",\n",
    "    \"learning_rate\": \"0.00256\",\n",
    "    \"dropout_rate\": \"0.045\",\n",
    "    \"early_stopping_patience\": \"10\"\n",
    "}\n",
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run DeepAR estimator job\n",
    "\n",
    "We are ready to launch the training job. SageMaker will start an EC2 instance, download the data from S3, start training the model and save the trained model.\n",
    "\n",
    "If you provide the test data channel, as we do in this example, DeepAR will also calculate accuracy metrics for the trained model on this test data set. This is done by predicting the last prediction_length points of each time series in the test set and comparing this to the actual value of the time series. The computed error metrics will be included as part of the log output.\n",
    "\n",
    "Note: the next cell may take a few minutes to complete, depending on data size, model complexity, and training options.\n",
    "\n",
    "<span style=\"color:red\">**For the Data Visualization Developer Workshop, we have pre-trained models in the interest of time.**</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_channels = {\n",
    "    \"train\": \"{}/train/\".format(s3_data_path),\n",
    "    \"test\": \"{}/test/\".format(s3_data_path)\n",
    "}\n",
    "\n",
    "#estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Trained DeepAR Model Endpoint\n",
    "Now that we have trained a model, we can use it to perform predictions by deploying it to an endpoint.\n",
    "\n",
    "**Note:** Remember to delete the endpoint after running this experiment. A cell at the very bottom of this notebook will do that: make sure you run it at the end.\n",
    "\n",
    "<span style=\"color:red\">**For the Data Visualization Developer Workshop, we have pre-created an endpoint in the interest of time.**</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#job_name = estimator.latest_training_job.name\n",
    "#\n",
    "#endpoint_name = sagemaker_session.endpoint_from_job(\n",
    "#    job_name=job_name,\n",
    "#    initial_instance_count=1,\n",
    "#    instance_type='ml.m4.xlarge',\n",
    "#    deployment_image=image_name,\n",
    "#    role=role\n",
    "#)\n",
    "\n",
    "endpoint_name = \"DataWorkshopEndpoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of the DeepARPredictor utility class. This will be used to help be used to make a request to the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = DeepARPredictor(\n",
    "    endpoint=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    content_type=\"application/json\"\n",
    ")\n",
    "predictor.set_prediction_parameters(time_freq, prediction_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_predict = [time_series_training[9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the previously created predictor object to generate the predicted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prediction = predictor.predict(time_series_predict, quantiles=[\"0.1\", \"0.5\", \"0.9\"])\n",
    "df_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Prediction \n",
    "\n",
    "Now that we have our prediction data, we will plot it against the test data.\n",
    "\n",
    "#### Steps\n",
    "\n",
    "\n",
    "1. Select the the resample interval, let we did early. Try between 1 and 6 hours. Also create a slice for the date range to plot. 2018-01-01 through 2018-01-07 is a good start. \n",
    "\n",
    "1. Create a dataframe from the prediction data based on the resample interval and date slice.\n",
    "\n",
    "1. Get the series for the .1, .5, and .9 quartiles.\n",
    "\n",
    "1. Setup plot area.\n",
    "\n",
    "1. Plot the median line (.5 quartile)\n",
    "\n",
    "1. Plot fill between the the .1 and .9 quartiles. This will give a shaded area that represents an 80% confidence interval.\n",
    "\n",
    "1. Create a dataframe from the actual test data, based on the resample interval and date slice.\n",
    "\n",
    "1. Plot the test data series\n",
    "\n",
    "1. Use `plt.legend()` to add and legend and `plt.show()` to output graph\n",
    "\n",
    "#### Notes\n",
    "https://matplotlib.org/api/colors_api.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 1 - parameters for time resolution and range to play\n",
    "plot_interval = '2h' #TODO: Try other intervals\n",
    "plot_slice = slice('2018-01-01','2018-01-07') #TODO: Try 14 days\n",
    "\n",
    "# Step 2 - Resampled and sliced prediction data\n",
    "toplot_dataframe = df_prediction[0][plot_slice].resample(plot_interval).sum()\n",
    "\n",
    "# Step 3 - Select quartiles to plot\n",
    "toplotmedian = toplot_dataframe['0.5']\n",
    "toplot10 = toplot_dataframe['0.1']\n",
    "toplot90 = toplot_dataframe['0.9']\n",
    "\n",
    "#Step 4 - Setup plot area. Let's make this one large.\n",
    "plt.figure(figsize=(20,7))\n",
    "\n",
    "#Step 5 - Plot median line and \n",
    "toplotmedian.plot(label='prediction median')\n",
    "\n",
    "#Step 6 - Plot confidence interval area\n",
    "plt.fill_between(toplot10.index, # x-axis\n",
    "                 toplot10,       # bottom y-axis\n",
    "                 toplot90,       # top y-axis\n",
    "                 color='y',      \n",
    "                 alpha=.5,       \n",
    "                 label='80% confidence interval'\n",
    ")\n",
    "\n",
    "#Step 7 - Create test data series\n",
    "actual_data_series = #TODO: resample and slice the time_series_test dataframe. See Step 2 for reference.\n",
    "\n",
    "#Step 8 - Plot actual data series\n",
    "#TODO: Plot the actual_data_series with a label of your choosing. See Step 5 for reference.\n",
    "\n",
    "#Step 9 - Add a legend and show the graph\n",
    "#TODO: Add a legend \n",
    "#TODO: Show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete endpoint\n",
    "\n",
    "<span style=\"color:red\">**Please do not delete the pre-created an endpoint!**</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sagemaker_session.delete_endpoint(...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
